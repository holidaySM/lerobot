{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04f27e0-bb17-4c9e-9caa-e1af5e4f964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9e8902-a198-483b-a7ca-848c427cbb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sm/anaconda3/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f113c5-a110-4cb1-a13c-1ec19db73aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available datasets:\n",
      "['lerobot/aloha_sim_insertion_human',\n",
      " 'lerobot/aloha_sim_insertion_scripted',\n",
      " 'lerobot/aloha_sim_transfer_cube_human',\n",
      " 'lerobot/aloha_sim_transfer_cube_scripted',\n",
      " 'lerobot/aloha_sim_insertion_human_image',\n",
      " 'lerobot/aloha_sim_insertion_scripted_image',\n",
      " 'lerobot/aloha_sim_transfer_cube_human_image',\n",
      " 'lerobot/aloha_sim_transfer_cube_scripted_image',\n",
      " 'lerobot/pusht',\n",
      " 'lerobot/pusht_image',\n",
      " 'lerobot/xarm_lift_medium',\n",
      " 'lerobot/xarm_lift_medium_replay',\n",
      " 'lerobot/xarm_push_medium',\n",
      " 'lerobot/xarm_push_medium_replay',\n",
      " 'lerobot/xarm_lift_medium_image',\n",
      " 'lerobot/xarm_lift_medium_replay_image',\n",
      " 'lerobot/xarm_push_medium_image',\n",
      " 'lerobot/xarm_push_medium_replay_image',\n",
      " 'lerobot/aloha_static_battery',\n",
      " 'lerobot/aloha_static_candy',\n",
      " 'lerobot/aloha_static_coffee',\n",
      " 'lerobot/aloha_static_coffee_new',\n",
      " 'lerobot/aloha_static_cups_open',\n",
      " 'lerobot/aloha_static_fork_pick_up',\n",
      " 'lerobot/aloha_static_pingpong_test',\n",
      " 'lerobot/aloha_static_pro_pencil',\n",
      " 'lerobot/aloha_static_screw_driver',\n",
      " 'lerobot/aloha_static_tape',\n",
      " 'lerobot/aloha_static_thread_velcro',\n",
      " 'lerobot/aloha_static_towel',\n",
      " 'lerobot/aloha_static_vinh_cup',\n",
      " 'lerobot/aloha_static_vinh_cup_left',\n",
      " 'lerobot/aloha_static_ziploc_slide',\n",
      " 'lerobot/aloha_mobile_cabinet',\n",
      " 'lerobot/aloha_mobile_chair',\n",
      " 'lerobot/aloha_mobile_elevator',\n",
      " 'lerobot/aloha_mobile_shrimp',\n",
      " 'lerobot/aloha_mobile_wash_pan',\n",
      " 'lerobot/aloha_mobile_wipe_wine',\n",
      " 'lerobot/aloha_static_battery',\n",
      " 'lerobot/aloha_static_candy',\n",
      " 'lerobot/aloha_static_coffee',\n",
      " 'lerobot/aloha_static_coffee_new',\n",
      " 'lerobot/aloha_static_cups_open',\n",
      " 'lerobot/aloha_static_fork_pick_up',\n",
      " 'lerobot/aloha_static_pingpong_test',\n",
      " 'lerobot/aloha_static_pro_pencil',\n",
      " 'lerobot/aloha_static_screw_driver',\n",
      " 'lerobot/aloha_static_tape',\n",
      " 'lerobot/aloha_static_thread_velcro',\n",
      " 'lerobot/aloha_static_towel',\n",
      " 'lerobot/aloha_static_vinh_cup',\n",
      " 'lerobot/aloha_static_vinh_cup_left',\n",
      " 'lerobot/aloha_static_ziploc_slide',\n",
      " 'lerobot/umi_cup_in_the_wild',\n",
      " 'lerobot/unitreeh1_fold_clothes',\n",
      " 'lerobot/unitreeh1_rearrange_objects',\n",
      " 'lerobot/unitreeh1_two_robot_greeting',\n",
      " 'lerobot/unitreeh1_warehouse',\n",
      " 'lerobot/nyu_rot_dataset',\n",
      " 'lerobot/utokyo_saytap',\n",
      " 'lerobot/imperialcollege_sawyer_wrist_cam',\n",
      " 'lerobot/utokyo_xarm_bimanual',\n",
      " 'lerobot/tokyo_u_lsmo',\n",
      " 'lerobot/utokyo_pr2_opening_fridge',\n",
      " 'lerobot/cmu_franka_exploration_dataset',\n",
      " 'lerobot/cmu_stretch',\n",
      " 'lerobot/asu_table_top',\n",
      " 'lerobot/utokyo_pr2_tabletop_manipulation',\n",
      " 'lerobot/utokyo_xarm_pick_and_place',\n",
      " 'lerobot/ucsd_kitchen_dataset',\n",
      " 'lerobot/austin_buds_dataset',\n",
      " 'lerobot/dlr_sara_grid_clamp',\n",
      " 'lerobot/conq_hose_manipulation',\n",
      " 'lerobot/columbia_cairlab_pusht_real',\n",
      " 'lerobot/dlr_sara_pour',\n",
      " 'lerobot/dlr_edan_shared_control',\n",
      " 'lerobot/ucsd_pick_and_place_dataset',\n",
      " 'lerobot/berkeley_cable_routing',\n",
      " 'lerobot/nyu_franka_play_dataset',\n",
      " 'lerobot/austin_sirius_dataset',\n",
      " 'lerobot/cmu_play_fusion',\n",
      " 'lerobot/berkeley_gnm_sac_son',\n",
      " 'lerobot/nyu_door_opening_surprising_effectiveness',\n",
      " 'lerobot/berkeley_fanuc_manipulation',\n",
      " 'lerobot/jaco_play',\n",
      " 'lerobot/viola',\n",
      " 'lerobot/kaist_nonprehensile',\n",
      " 'lerobot/berkeley_mvp',\n",
      " 'lerobot/uiuc_d3field',\n",
      " 'lerobot/berkeley_gnm_recon',\n",
      " 'lerobot/austin_sailor_dataset',\n",
      " 'lerobot/utaustin_mutex',\n",
      " 'lerobot/roboturk',\n",
      " 'lerobot/stanford_hydra_dataset',\n",
      " 'lerobot/berkeley_autolab_ur5',\n",
      " 'lerobot/stanford_robocook',\n",
      " 'lerobot/toto',\n",
      " 'lerobot/fmb',\n",
      " 'lerobot/droid_100',\n",
      " 'lerobot/berkeley_rpt',\n",
      " 'lerobot/stanford_kuka_multimodal_dataset',\n",
      " 'lerobot/iamlab_cmu_pickup_insert',\n",
      " 'lerobot/taco_play',\n",
      " 'lerobot/berkeley_gnm_cory_hall',\n",
      " 'lerobot/usc_cloth_sim']\n"
     ]
    }
   ],
   "source": [
    "print(\"List of available datasets:\")\n",
    "pprint(lerobot.available_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2412550b-4f17-4815-94b7-5ec1064425d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 806 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:00<00:00, 30911.25it/s]\n"
     ]
    }
   ],
   "source": [
    "repo_id = 'lerobot/xarm_lift_medium'\n",
    "dataset = LeRobotDataset(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705d08a5-f77f-4151-867e-6f6030c7ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'lerobot/xarm_lift_medium',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 20000,\n",
      "  Number of Episodes: 800,\n",
      "  Type: video (.mp4),\n",
      "  Recorded Frames per Second: 15,\n",
      "  Camera Keys: ['observation.image'],\n",
      "  Video Frame Keys: ['observation.image'],\n",
      "  Transformations: None,\n",
      "  Codebase Version: v1.6,\n",
      ")\n",
      "Dataset({\n",
      "    features: ['observation.image', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'index'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a139eb-b037-46fb-a625-4fe97292401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average number of frames per episode: 25.000\n",
      "frames per second used during data collection: dataset.fps=15\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.image']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deff9574-7a4f-4c6f-85ee-27b1961b9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 start from index 0 to index 25\n"
     ]
    }
   ],
   "source": [
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "print(f\"episode {episode_index} start from index {from_idx} to index {to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0acd415-9963-4a20-9e0d-0935a20475ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dataset[idx][\"observation.image\"] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# Video frames are now float32 in range [0,1] channel first (c,h,w) to follow pytorch convention. To visualize\n",
    "# them, we convert to uint8 in range [0,255]\n",
    "frames = [(frame * 255).type(torch.uint8) for frame in frames]\n",
    "# and to channel last (h,w,c).\n",
    "frames = [frame.permute((1, 2, 0)).numpy() for frame in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa064dbd-e520-4159-9881-6c5991d913a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (84, 84) to (96, 96) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x64eabc0] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    }
   ],
   "source": [
    "# Finally, we save the frames to a mp4 video for visualization.\n",
    "Path(\"outputs/examples/1_load_lerobot_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "imageio.mimsave(f\"outputs/examples/1_load_lerobot_dataset/episode_{episode_index}.mp4\", frames, fps=dataset.fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4835a-9ab9-4d09-b0eb-d50181f8f5c2",
   "metadata": {},
   "source": [
    "# Example Data Processing\n",
    "\n",
    "For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "differences with the current loaded frame. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f3e500-da8d-4f9a-8e61-8f292f73775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_timestamps = {\n",
    "    # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "    \"observation.image\": [-1, -0.5, -0.20, 0],\n",
    "    # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 20 ms, 10 ms, and current frame\n",
    "    \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, -0.02, -0.01, 0],\n",
    "    # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "    \"action\": [t / dataset.fps for t in range(64)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc660388-3b29-4e0f-964a-069ae5f48764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 806 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 806/806 [00:00<00:00, 22509.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset[0]['observation.image'].shape=torch.Size([4, 3, 84, 84])\n",
      "dataset[0]['observation.state'].shape=torch.Size([8, 4])\n",
      "dataset[0]['action'].shape=torch.Size([64, 4])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "print(f\"\\n{dataset[0]['observation.image'].shape=}\")  # (4,c,h,w)\n",
    "print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e845527-e92d-437b-8d9f-676cbae708f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['observation.image'].shape=torch.Size([32, 4, 3, 84, 84])\n",
      "batch['observation.state'].shape=torch.Size([32, 8, 4])\n",
      "batch['action'].shape=torch.Size([32, 64, 4])\n"
     ]
    }
   ],
   "source": [
    "# Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# PyTorch datasets.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "for batch in dataloader:\n",
    "    print(f\"{batch['observation.image'].shape=}\")  # (32,4,c,h,w)\n",
    "    print(f\"{batch['observation.state'].shape=}\")  # (32,8,c)\n",
    "    print(f\"{batch['action'].shape=}\")  # (32,64,c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4efe0-398c-455f-9e29-3643631bb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
